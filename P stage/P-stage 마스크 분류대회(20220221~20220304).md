# P-stage 마스크 분류대회(20220221~20220304)

# 결과

- **public → 1 / 48등**
    - **f1:  0.8022, acc:83.7132**
- **private → 1 / 48등**
    - **f1:  0.7959 , acc: 83.0000**
- [깃허브](https://github.com/boostcampaitech3/level1-image-classification-level1-nlp-03)

# TIL

- 텐서보드(Tensorboard)
    - smoothing 옵션 0으로 줘야 확실한 값 볼 수 있음!
    - grid_image의 인자 n 수정하면 텐서보드 상에 뜨는 이미지 숫자 늘릴 수 있음!
    
    ```python
    def grid_image(np_images, gts, preds, n=32, shuffle=False):
    ```
    
    - local 말고 조원과 공유하려면 (**🙏시현님 , [페북링크](https://www.facebook.com/groups/TensorFlowKR/posts/1030430097298048/))**
    
    ```bash
    root@ : ~/model# tensorboard --logdir . --bind_all
    ```
    
- GPU서버 vscode연결(**🙏[현지님](https://www.notion.so/AIStage-3e8ac3b8b8104628a0a6df971b50fa39))**
    - 겪은 오류
        - 세펴레이션 : / → \\ 로 해야 함!
        - Could not establish connection to ~ip주소 : 경로에 한글 들어가지 않게 + .ssh 파일에 넣음!
        - Connect to Host(Windows) 할 때 Enter  password : Linux로 하면 됨..
- tqdm
    - for idx, train_batch in enumerate(train_loader): → for idx, train_batch in enumerate(tqdm(train_loader)):
    - inference 시에 tqdm가능! (inference.py에 import 하고 train.py에서와 마찬가지로 enumerate(tqdm(loader)) 하면 됨!)
    - tqdm(enumerate(train_loader)): 하면 정보 출력 잘 안됨!
        - [**참고 글**](https://jangjy.tistory.com/395)
    
    ```python
    from tqdm import tqdm
    for idx, train_batch in enumerate(tqdm(train_loader)):
    ```
    
- IDE 단축키(vscode)
    - ctrl + 클릭 : 내부코드 볼 수 있음!
    - (ctrl + k) + (ctrl + s) → 바로가기 키 수정
    - ctrl + p : 네비게이션
    - bookmarks : extension 에서 bookmarks 설치
        - ctrl+alt+k → 북마크 지정,취소
        - ctrl+alt+L → 다음꺼 탐색
        - ctrl+alt+J → 전 꺼 탐색
        - 오류 : 북마크 지정하면 같은.py파일 안에서만 이동가능했었음(>Bookmarks: List from All Files 실행 시 안 뜸 ) → 파일 열기로 workspace 지정하니 됨!(파일 간 이동가능)
- torchvision.transforms
    - p를 인자로 받는 경우, 모든 이미지에 적용되는 것이 아닌 확률적으로! Ex: verticalflip
    - 확률인자가 없다면 모두 적용 Ex: RandomCrop
- 깃허브
    - octotree : 깃허브 저장소의 폴더,파일 구조가 트리 형태로 쉽게 볼 수 있음!(**🙏[링크](https://tttsss77.tistory.com/55))**
    - 파일 한 개 다운로드 : 깃허브 Raw → ctrl+s (xml은 되는데, ~~ipynb는 text로 받아짐..~~ —> 파일형식 Text Documnet → 모든 파일 로 하면 됨!)
    
    ![https://user-images.githubusercontent.com/46811558/156138811-3f9adb3f-4856-4178-b837-7ed88e851ae8.JPG](https://user-images.githubusercontent.com/46811558/156138811-3f9adb3f-4856-4178-b837-7ed88e851ae8.JPG)
    
- 파이썬
    - pprint : 이쁘게 출력!
- 노션
    - ~ 글 ~ : 공백 지우면 글에 취소선 Ex: ~~취소선~~
- GPU
    - 터미널에서 nvidia-smi **([표 보는 법 링크](https://kyumdoctor.co.kr/10))**
        - [**(MiB vs MB 참고링크)**](https://brunch.co.kr/@leedongins/133)
    - 노트북 환경
    
    ```python
    import torch
    torch.cuda.is_available()
    ```
    
- 모델
    - SOTA 찾기
        - [**timm**](https://github.com/rwightman/pytorch-image-models#introduction)
        - [**paperswithcode**](https://paperswithcode.com/)
- 랜덤시드(**🙏인식님)**
    - 모델의 성능을 동일하 조건에서 동일하게 복구 할 수 있어야 함! 실험 관리에 굉장히 중요!
    - 관례상 **42**로 고정! [**(참고 글)**](https://rchoi-19-4-2.tistory.com/159)
- 앙상블
    - 하드보팅 : label로 앙상블(csv로..?)
    - 소프트보팅 : 확률로 앙상블
    - Weighted 보팅 : 앙상블 할 때,  각 모델별로 가중치 줌
- 기타
    - ctrl + shift + r : 페이지 캐시 지우기
- nohup
    - no hang up!
    - [**참고링크**](https://joonyon.tistory.com/98)
- tmux
    - Terminal Multiplexer
    - [**참고링크**](https://hbase.tistory.com/200)
- bash
    - Experiment.txt 파일에서 한 줄씩 일일이 복붙하는것보단 세연님이 만들어주신 train.sh파일 사용하는 것이 관리측면에서 훨씬 좋음!
        
        ```bash
        bash train.sh
        sh train.sh
        ```
        
        - bash train.sh  , sh train.sh 둘 다 가능!
            - [**bash vs sh 참고링크**](https://www.google.com/search?q=bash+vs+sh&sxsrf=APq-WBuEzeDO9Uy-nVDk1tLGltqUunNl_A%3A1646798653081&ei=PScoYvGPBJDR2roPlJ-X2A0&ved=0ahUKEwixou6Fk7j2AhWQqFYBHZTPBdsQ4dUDCA4&uact=5&oq=bash+vs+sh&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEIAEMgUIABCABDIFCAAQgAQyBQgAEIAEMgUIABCABDIFCAAQgAQyBQgAEIAEMgQIABAeMgUIABDLATIFCAAQywE6BwgjEOoCECc6BAgjECc6EQguEIAEELEDEIMBEMcBENEDOg4ILhCABBCxAxDHARCjAjoICC4QgAQQsQM6CwgAEIAEELEDEIMBOgoIABCABBCHAhAUOggIABCABBCxAzoECC4QAzoNCAAQgAQQhwIQsQMQFDoQCAAQgAQQhwIQsQMQgwEQFEoECEEYAEoECEYYAFAAWPIUYOQVaAVwAHgBgAG1AYgBhQuSAQQwLjEwmAEAoAEBsAEKwAEB&sclient=gws-wiz)
    - 리눅스 명령어 이용하면, 파일 순차적으로 실행하면 시간 훨씬 절약 가능
    - [**참고링크**](https://velog.io/@jekim5418/Shell-Script-vi-%EC%9E%91%EC%84%B1)
- 터미널 실행종료
    - ctrl + c
    - 터미널 휴지통!
    
    ![https://user-images.githubusercontent.com/46811558/156290480-bba4bbe6-b899-41a4-b74e-abaf5c14a31e.JPG](https://user-images.githubusercontent.com/46811558/156290480-bba4bbe6-b899-41a4-b74e-abaf5c14a31e.JPG)
    

# 모델

- baseline : SwinTransformerLarge384(🙏세연님)
    - [모델](https://github.com/boostcampaitech3/level1-image-classification-level1-nlp-03/tree/main/PSY)(Github)
        - [**pretrained Models 다운로드 링크**](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth)
    - 설명
        - [유튜브1 :](https://www.youtube.com/watch?v=L3sH9tjkvKI&t=3s) 논문스터디 리뷰, 20분, 1.25배, 글과 비슷
        - [유튜브2 :](https://www.youtube.com/watch?v=2lZvuU_IIMA) DSBA연구실 리뷰, 40분, 1.5배, vit → swin 발전 과정
        - [글](https://visionhong.tistory.com/31)

# baseline 변경

- dataset.py
    - ageband : (30,60) → (29,57), (29,58) , (29,59) (**🙏태일님)**
    
    ![https://user-images.githubusercontent.com/46811558/156015632-b30a6bd2-aa0d-4ba3-8c6f-0a2dfd8fb7db.JPG](https://user-images.githubusercontent.com/46811558/156015632-b30a6bd2-aa0d-4ba3-8c6f-0a2dfd8fb7db.JPG)
    
    - random.choices(복원 추출) → random.sample(비복원 추출)  (**🙏상렬님,주영님)**
        - set을 써서 중복은 제거 되겠지만.. 제거된 만큼 소중한 val이 사라져버림..
    
    ![https://user-images.githubusercontent.com/46811558/156016243-4f2a65fa-ecda-4a84-8f5c-c9d0fa3f5db0.JPG](https://user-images.githubusercontent.com/46811558/156016243-4f2a65fa-ecda-4a84-8f5c-c9d0fa3f5db0.JPG)
    
- inference.py
    - resize : (96,128 : 기존) → (128,96 : train dafault) → (384,384 : swin 모델)
    
     
    
    ![https://user-images.githubusercontent.com/46811558/156017503-0d9bce99-f597-42ec-a764-f7e597c606b3.JPG](https://user-images.githubusercontent.com/46811558/156017503-0d9bce99-f597-42ec-a764-f7e597c606b3.JPG)
    
- train.py
    - 어차피 명령어로 바꿀 수 있음!
    
    ![https://user-images.githubusercontent.com/46811558/156018068-f7768e3d-62a1-4265-8fbc-8bce7bbcc519.JPG](https://user-images.githubusercontent.com/46811558/156018068-f7768e3d-62a1-4265-8fbc-8bce7bbcc519.JPG)
    
- loss.py
    - F1 loss 를 metric으로 사용하기 위해 classes=3 →18 수정!
        - 대회 score가 f1이니 f1으로 train하면 되지 않을까 ..? 라는 생각에서
        - 작으면 된다니까 2는..? → 안됨!
    - 수정하지 않을 경우(오류)
        - class values must be smaller than num classes (세연님 베이스라인 이전 모델)
        - The size of tensor a (n) must match the size of tensor b (18) at non-singleton dimension 1 (세연님 베이스라인 모델, n : 정의된 classes)
    
    ![https://user-images.githubusercontent.com/46811558/156019099-8c5b022c-2ef8-4ef6-9493-03909070b454.JPG](https://user-images.githubusercontent.com/46811558/156019099-8c5b022c-2ef8-4ef6-9493-03909070b454.JPG)
    

# Loss 설명

- Loss 선정
    - Cross-Entropy : 분류 문제에서 흔하게 사용
    - Focal Loss : 확률이 높은 케이스에는 확률이 낮은 케이스보다 Loss를 더 크게 낮추는 보상 → 우리의 데이터에 맞게 Class Imbalance에 적격!
    - LabelSmoothingLoss : Mislabeling data를 고려하기에, 모델 일반화에 좋은 성능
    - F1 loss : 대회에서 Macro F1 score를 사용하여 순위 산정

![F1 score 수식 , [**AI stage(평가 방법](https://stages.ai/competitions/104/overview/evaluation))**](https://user-images.githubusercontent.com/46811558/156926611-69ad95f6-94e2-4790-8f4d-a4629be952ed.JPG)

F1 score 수식 , [**AI stage(평가 방법](https://stages.ai/competitions/104/overview/evaluation))**

위의 이미지는 F1 score의 수식으로, 대회에서 사용하는 Macro F1 score는 각 클래스 값에 대해 별도의 F1 score를 계산 한 다음 평균을 낸다. F1 loss를 선택한 이유는,대회에서 Macro F1 score를 사용하여 순위를 산정하기에 대회의 취지와 맞지 않을까 하는 생각과, train 과정에서 나오는 많은 모수들이 중심극한 정리를 통해 정규분포로 근사화되고, 이를 음수화한다음 1을 더해줄경우, 확률로 가정할 수 있다는 생각으로 단순하게 접근하였다. 

이후에는 수식에 대해서 공부했고, [**캐글](https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric)** 과 **[기술블로그](https://towardsdatascience.com/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d)** 에 따르면, loss를 metric으로 쓰는 것은 최고의 선택이며, 임계값을 지정하거나 F1 score를 loss 함수에 포함시키면 성능을 향상시킬 수 있다는 근거가 뒷받침 되어있다. 하지만 일반적으로 잘 쓰이지 않는데, 그 이유는 f1 loss함수는 미분이 불가능한 cost function이기 때문이다. 하지만, 주어진 F1 score 식을 0과 1의 정수값 예측이 아닌 확률 값으로 대신하여 예측할 경우 미분이 가능하게 할 수 있다. 

예를 들어, ground truth가 1이고 모델의 예측이 0.4일 경우, 우리는 0.4의 true positive와 0.6의 false negative를 얻을 수 있고, 만약 ground truth가 0이고 모델의 예측이 0.4일 경우, 0.6의 true negative와 0.4의 false positive 값을 얻을 수 있을 것이다. 이러한 확률예측을 통해 f1 loss함수를 밑의 그림과 같이 미분이 가능하게 만들 수 있다.

![Macro F1 score 함수 개형 , [**기술 블로그**](https://towardsdatascience.com/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d)](https://user-images.githubusercontent.com/46811558/156926578-dd03d9f5-6662-4c3c-b43b-e11451cb7530.JPG)

Macro F1 score 함수 개형 , [**기술 블로그**](https://towardsdatascience.com/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d)

또, baseline의 loss.py파일에 있는 loss식을 참고하면, return 값이 1-f1.mean()이기에 (1-f1)을 최소화 한다는 것은 f1 score를 최대화 하는것과 같기에, 대회의 취지에 맞을 것이고 실제로 성능향상에 큰 도움을 주었다.

# 데이터

- 개인
    - 데이터 (**🙏시현님,효석님,영석님,주영님) → 토론게시판 EDA 및 팀 노션 참고!**
        - 1498-1 , 4432 : male → female
        - 6359 ~ 6364 : female → male
        - 20, 4418, 5227 : normal ↔ incorrect
        - 건드리고 싶지만, 어떻게 건드려야 할지도 모르겠는 데이터가 많았다..
            - mask 쓴거같은데 incorrect..
        - 참고
            - [**토론게시판](https://stages.ai/competitions/104/discussion/talk) ,**      ⬇️
                
                [Mask Classfication Dataset EDA](https://www.notion.so/Mask-Classfication-Dataset-EDA-dcb4f73dea3d450da0305eea2b12a608)
                
- 팀(SOTA)
    - 배경 제거 데이터(**🙏상렬님)**
    - 구글 드라이브(저작권 상 비공개)

# 제출 표

- [**Ai 스테이지**](https://stages.ai/competitions/104/submission/my)
- 항목 : 제출번호, 모델 , 메모, 제출 score(f1, acc) ,acc(train,val) , loss(train,val) , config.json, 아쉬운 점 + 느낀 점

| 제출 번호 |          모델  |                 메모  |             f1                  (public → private) |            acc                            (public → private) |        acc            (train  ,  val) |       loss                (train  ,  val) |                              config.json                                                                                              　              (SOTA 비교하여 다른 점)  |     아쉬운 점 + 느낀 점 |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|    1 |         Eff b6 | train,val 그래프는 좋던데.. | 0.4896 → 0.4872 | 63.2063 → 61.7619 | 90  ,  89.31 | 기록 지움ㅠ | batch : 64(valid 도)  , optm : Adam ,epoch : 10, loss : focal , lr : 1e-5 | 이미지 사이즈(논문에 맞추지 않고 아마 128,96?) |  |
|    2 |         Eff b7 | 오버피팅 끝판왕..; | 0.6077 → 0.5864 | 68.2063 → 67.7619 | 99  ,  96.80 | 기록 지움ㅠ | (1) + epoch : 40 | (1)와 같음 |  |
|    3 |         Eff b7 | custom augmentation 추가 + age band : 25,55 | 0.5891 → 0.5584 | 64.4286 → 63.7143 | 99  ,  92.83 | 기록 지움ㅠ | (2) + custom Augmentation | (1)와 같음 + (25,55)는 아니군! |  |
|    4 |         Eff b7 | Base augmentation 그대로 + age band : 27,57 | 0.6332 → 0.5983 | 68.3968 → 66.5238 | 기록 지움ㅠ | 기록 지움ㅠ | (2) + batch : 32 | (1)와 같음 + (27,57)은 그나마..? |  |
|    5 |          Vit  |    age band : 27,57  | 0.2781 → 0.2719 | 37.0635 → 35.6190 | 기록 지움ㅠ | 기록 지움ㅠ | (1) + epoch : 3 + resize : 224,224 | 다른 분들은 잘 되던데.. |  |
|    6 | Swin Transformer | age band : 29,57(🙏태일님)     choices → sample(🙏주영님)  여기까지 (base_v2) | 0.6621 → 0.6527 | 73.8571 → 74.3016 | 기록 지움ㅠ | 기록 지움ㅠ | epoch : 1 , dataset : MaskSplitByProfileDataset, augmentation : CustomAugmentation , batch : 16 , optm : AdamW ,lr : 2e-5, loss : f1 , lr_decay_step : 4 ,val_ratio : 0.2 | epoch 왜 1로했지.. |  |
|    7 | Swin Transformer + Lightning | 이제부턴 (6)과 동일(🙏세연님)  | 0.7763 → 0.7623 | 81.7619 → 81.1270 | 87  ,  83 | 0.57  ,  0.38 | (6) + epoch : 5 | 최고 기록 달성! |  |
|    8 | Swin Transformer + Lightning |  | 0.7594 → 0.7402 | 79.8730 → 79.3333 | 96  ,  83 | 0.53  ,  0.37 | (6) + epoch : 15 | 오버피팅의 표본.. |  |
|    9 | Swin Transformer + Lightning |  | 0.7594 → 0.7402 | 79.8730 → 79.3333 | 96  ,  85 | 0.53  ,  0.36 | (6) + epoch : 13 | 15번 돌리면 오버피팅 나구나.. 13번 돌린다면 ..? → 여전히 오버피팅, 제출할 때 비교해보고 내자.. |  |
|   10 | Swin Transformer + Lightning |  | 0.7414 → 0.7281 | 78.8413 → 79.0000 | 93  ,  83 | 0.5  ,  0.37 | (6) + epoch : 7 | 7번 돌리면 더 낮게 나오네 ..? → 실시간으로 best.pth 파일 확인해야겠네 |  |
|   11 | Swin Transformer + Lightning |  | 0.7763 → 0.7623 | 81.7619 → 81.1270 | 92  ,  83 | 0.57  ,  0.38 | (6) + epoch : 6 | 무지성으로 6번돌림(자정 직전 기회 남음) → 분석 후 제출하자..그래프 봤으면 3번 돌린게 최적인 걸 알았을텐데.. |  |
|   12 | Swin Transformer + Lightning | age band 실험 → 29,58 | 0.7808 → 0.7741 | 82.0635 → 81.9524 | 90  ,  86 | 0.57  ,  0.38 | (6) + epoch : 5 + ageband : 29,58 | 초심으로 돌아와 기본세팅 + age밴드 수정 → 기록 갱신, train vs val 비교해서 그래프만 봐도, ‘이게 일반화가 잘된거구나’ 바로 알 수 있음(train vs val의 그래프 간격 차이) |  |
|   13 | Swin Transformer + Lightning | age band 실험 → 29,59 | 0.7727 → 0.7651 | 82.3175 → 81.9683 | 91  ,  89 | 0.55  ,  0.3867 | (6) + epoch : 5 + ageband : 29,59 | 내 모델 기준 29,58이 최적이겠군! |  |
|   14 | Swin Transformer + Lightning | age band 29,57로 회귀(age band의 최적을 알았으니.. 다른것을 조절해볼까? → epoch:3) | 0.7763 → 0.7623 | 81.7619 → 81.1270 | 86  ,  83 | 0.60  ,  0.42 | (6) + epoch : 3 | 무지성 실험 끝 . 실험관리의 중요성 → 하나를 변경했으면 나머지도 다시 변경해야 하나? → 그러지말고 AutoML 또는 최적의 파라미터 세팅후에 비교하자(epoch은 그래프만 봐도 아니까) + 이때 머리 복잡하기 시작 |  |
|   15 | Swin Transformer + Lightning | 최적으로회귀(12) + val_ratio 팀원끼리 다른거 확인하고 0.2 →0.1 | 0.7679 → 0.7430 | 81.0317 → 80.3175 | 90  ,  86 | 0.56  ,  0.38  | (12) + val_ratio : 0.2 → 0.1 | 태일님 데이터의 경우엔 샘플링을 통해 학습데이터 부족 → val ratio 가 0.1인게 적합, 하지만 내 데이터의 경우는 충분하기에 val ratio를 줄이면 오버피팅이 일어날 수 있음!(f1 score 감소한게 큼), 하지만 실험관리 보면서 꼼꼼하게 다른 점 파악한 건 잘함! |  |
|   16 | Swin Transformer + Lightning | (15) + epoch 8 | 0.7723 → 0.7585 | 81.4762 → 81.3016 | 94  ,  87 | 0.55  ,  0.40 | (12) + val_ratio : 0.2 → 0.1 + epoch : 5 → 8 | val ratio를 줄이고 epoch을 8로 늘리면, 오히려 오버피팅이 날거라고 생각했는데, 왜 더 늘지..? 알다가도 모르겠네 |  |
|   17 | Swin Transformer + Lightning | 최적으로회귀(12) + Test Data Center Crop | 0.7822 → 0.7731 | 81.6508 → 81.5397 | (12) 그대로 →test data set만 수정했으니 | (12) 그대로 →test data set만 수정했으니 | (12) + Test Data Set Center Crop | 대회마다 다르겠지만, 이번 대회의 경우는 암호화된 파일로 테스트데이터 제공 → 살펴본후에 학습데이터와 다를게없네? → 데이터전처리 학습과 동일하게 해도 될듯! → 개인 최고기록 갱신! |  |
|   18 | Swin Transformer + Lightning | 최적으로회귀(12) + Test Data에 Center Crop, Color Jitter | 0.7776 → 0.7670 | 81.2857 → 81.0794 | (12) 그대로 →test data set만 수정했으니 | (12) 그대로 →test data set만 수정했으니 | (12) + Test Data Set Center Crop + Test Data Set Color Jitter | 센터크롭이 성공했으니, 테스트 데이터를 트레인 데이터와 아예 같게..? → 컬러지터는 안되네.. 어떻게 해석해야 하나(평가 데이터에 노이즈 x..? 학습 데이터와 비슷하다 ..?) |  |

**<실험 메모 → 위의 표 기준>**

17번 : 2958nogaussianval0.2e5t_ccrop

15번 : 2958val0.1 

16번 : 2958val0.1epochs8real 

제출x실험만 :  2960val0.2nogaussian → age : 29,60 + SOTA 동일

제출x실험만 :  addgaussiannoise → train 시에 AddGaussiannoise 추가 + SOTA동일

12번 : f1loss_exp19=exp19

제출x실험만 :  focal → SOTA 동일 + loss만 focal → 발표자료용

---

exp 2             : epochs   5,  age : 29,57  —> 0.7763_81.7619

exp 13           : epochs   6,  age : 29,57  —> 0.7763_81.7619

exp 17(exp9) : epochs   7 , age : 29,57  —> 0.7414_78.8413

exp 4             : epochs 15 , age : 29,57  —> 0.7594_79.8730

exp 5             : epochs 13 , age : 29,57  —> 0.7594_79.8730

exp 19           : epochs 5   , age : 29,58  —> 0.7808_82.0635

exp 20           : epochs 5   , age : 29,59  —> 0.7727_82.3175

exp 27           : epochs 3   , age : 29,57  —> 0.7763_81.7619

---

# 실험

- **실험1🙏태일님**
    - epochs 5 → ageband 수정 (29,57) vs (29,58) vs (29,59)
        - (30,60) → ( 25,27,28,29,30 中 1개  , 55,58,59,60 中 1개)
        - SOTA 기준 29,58 이 제일 잘 나옴 : 0.7822 ,81.6508
    
    ![https://user-images.githubusercontent.com/46811558/156181012-7989f23a-5b2d-44d7-afac-c04e76e6a3d2.JPG](https://user-images.githubusercontent.com/46811558/156181012-7989f23a-5b2d-44d7-afac-c04e76e6a3d2.JPG)
    
    - 제출한 결과를 보면 , val의 결과 그대로 따라가는 듯하다. exp20을보면 제출결과 acc는 제일높고, f1은 제일 낮은데 실제 제출 결과도 그러하다.
- **실험 2🙏태일님**
    - age band (29,57) 고정 → epochs 수 변화
        - 결과 : 거기서 거기~
        - 태일님 의견 듣고 loss로 기준 잡음
        - epochs : 3,5,6,7,8,13,15 비교
        - 밑의 이미지는 가장 긴 epoch 15 → 많이 할 수록 복잡한 그래프 → 오버피팅
            - 성능향상이 있었던 실험마다 epoch변화해서 제출하는 것보다 그래프 봐가며 best.pth 시간 확인하고 성능확인(csv비교)후에 제출했어야함.. ~~제출2번날림~~
            - 결론적으로 epoch 3에서 이미 최고점!
        
        ![https://user-images.githubusercontent.com/46811558/156908331-c2d3e014-2d28-4987-bd2d-891b17041827.JPG](https://user-images.githubusercontent.com/46811558/156908331-c2d3e014-2d28-4987-bd2d-891b17041827.JPG)
        
- **실험 3🙏나^^**
    - age band(29,58) 고정 →  val_ratio 변화 (0.1 vs 0.2)
        - 다른 분들 config.json 에서 다른 점 발견! **(뿌듯)**
        - 기록 지워서, val 0.1로한건 epoch 8로 비교
            - 태일님 기록갱신 같은 경우는, 다운샘플링으로 인한 데이터부족 → 0.1이 효과 O
            - 내 데이터는 , 원본 데이터이다보니 데이터갯수 충분→ 0,1이 효과 X
        
        ![https://user-images.githubusercontent.com/46811558/156908437-47b8d710-f308-4348-9b72-a4f6b19eefd1.JPG](https://user-images.githubusercontent.com/46811558/156908437-47b8d710-f308-4348-9b72-a4f6b19eefd1.JPG)
        
- **실험 4**
    - 배경제거X모델 SOTA고정 → train에만 AddGaussiannoise
        - 조원분들이 실험 후 제출 → 성능 상승 X →제출 X
        - 실제로 밑의 그래프만 봐도 좋지 않아보임..→ 개형은 이쁜데.. 혹시 제출했다면 일반성을 높였을수도?
        
        ![https://user-images.githubusercontent.com/46811558/156908547-748d4590-8f18-434b-ab7c-c05d5452ec14.jpg](https://user-images.githubusercontent.com/46811558/156908547-748d4590-8f18-434b-ab7c-c05d5452ec14.jpg)
        
- **실험 5🙏규범님**
    - 배경제거X모델 SOTA고정 → test에 CenterCrop 추가!
        - 테스트 데이터만 건드렸으므로, 학습 그래프는 동일!
        - 0.7808 → 0.7822로 개인최고기록 갱신!
        - 테스트 데이터가 학습데이터랑 상당히 비슷하군!
- **실험 6**
    - 배경제거X모델 SOTA고정 → test에 CenterCrop+Color Jitter 추가!(train Augmentation과 동일 환경)
        - 테스트 데이터만 건드렸으므로, 학습 그래프는 동일!
        - 0.7808 → 0.7776으로 다운그레이드..
        - 테스트 데이터가 학습데이터랑 상당히 비슷하지만, Color Jitter는 학습시에만 써야하겠군!
- **실험 7**
    - 배경제거X모델 SOTA고정  → random seed 바꾸기 (운빨!)
        - 매번 다를 듯..?

# 시도

- 모델
    - vgg19bn
        - 처음으로 적용이 가능한 모델 이었지만, 성능이 그렇게 좋진 않음
    - efficientnet b4,b6,b7
        - acc기준 70%전후로 처음 접하게 해준 모델
    - Vit
        - 생각보다 잘 안나옴..
    - Swin transformer + Pytorch Lightning
        - 모델 최고!
        - 연산 속도 반토막으로 줄여줌
- loss
    - focalloss
        - 불균형한 클래스의 데이터에 효과적이라 했지만 , f1 loss가 훨씬 잘 나옴
    - Label Smoothing loss
        - baseline code에는 smoothing 0 → Cross Entropy와 다를게없음
        - f1 loss가 더 잘 나옴!
- 데이터
    - age band 수정
        - (30,60) → ( 25,27,28,29,30 中 1개  , 55,58,59,60 中 1개)
            - SOTA 기준 29,58 이 제일 잘 나옴 : 0.7808 ,82.0635
- 데이터셋
    - 배경제거 → 상렬님 참고
    - EDA 후
        - mislabeling 수정
        - 애매한 부분은 그냥 건드리지 않음!

# 회고

## <느낀 점>

- 텐서보드 못 믿겠다.. effb7했을 때, train 99 val 97 나오길래 기록 갱신 할 줄..
    - (기록 지워서 못 보지만) 분명 overfitting 일어나서 , epochs 30으로 해도 10쯤에 저장되었을 수도
    - pth파일 저장되는 시간을 잘 봤어야 함. epochs 무지성으로 늘린다고 효율적이진 않음.
- Pytorch Lite 쓰고나서 내 생각에는 같은 세팅이라 생각했지만, 속도도 빠르고 성능도 상승
- 성능이 잘나와도 머리가 아프다.
    - 왜 잘 나오는지? → XAI 공부하기..(Gradient Cam) **🙏규범님**
    - 성능을 더 높이기 위해 인자들 수정 (optimizer,batch size,lr,epoch, step,decay 등등 너무 많음) → 지수 승으로 늘어나니 .. AutoML 필수다..
- 제출 할 때 annotation 필수!
    - 바보 같이 똑같은 제출 2번이나 함.. ~~Sorry to 내가네가..~~
    - SOTA 모델과 검증 해주는 코드 절실!**🙏시현님**

## <높은 등수의 요인(개인적인 생각→발표 자료 목차로 !)>

- 프로그램
    - Swin Transformer
    - f1 loss
    - Pytorch Lite
- 데이터
    - 배경제거
    - EDA를 통한 mislabeling 해결
    - age band 수정 및 샘플링
    - random.choices → random.sample
- 커뮤니케이션
    - 실험 결과 공유
    - 코드 공유
    - 질의응답

## <궁금증>

- 베이스라인을 만드신 분들의 여러 코드를 넣은 이유?
- 텐서보드 상의 학습 그래프에서 early stopping 기준 : **accuracy? vs loss?**
- 왜 test size (96,128) ?
- 최종제출 일반적인 관행?
    - SOTA 1
    - 앙상블(Hard Voting) →상위 8개
    - ~~배경제거하지 않은 내 SOTA(ㄲㅂ..)~~

## <해결하지 못한 이슈>

- 모델별 성능
    - vgg 모델은 학습시에도 큰 성과 X
    - EfficientNet-b7의 경우 epoch30번 → train : 99, val : 97
        - best.pth를 확인해가며 학습하지 못한 잘못 도 있지만, 오버피팅의 이슈..?
- dataset.py의 mean,std 값의 의미 ..?
    - 이미지넷
        - mean = [0.485, 0.456, 0.406]
        - std = [0.229, 0.224, 0.225[
    - 마스크 데이터 default
        - mean = [0.548, 0.504, 0.479]
        - std = [0.237, 0.247, 0.246]
    - 배경 제거 데이터(train) **🙏상렬님**
        - 전체 길이 : 18900
        - mean = [0.20629331, 0.17723827, 0.16767759]
        - std = [0.34968819, 0.30846628, 0.29714536]
    - 배경 제거 데이터(eval) **🙏상렬님**
        - 전체 길이 : 12600
        - mean = [0.20683326, 0.16344436, 0.15733106]
        - std = [0.35039557, 0.28900916, 0.27917677]
    - 원본 데이터
        - 구해보진 않았지만, 아마 마스크 데이터의 default일 듯(test 데이터만 해봤는데 비슷하게 나옴)
