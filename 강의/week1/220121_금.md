# 이재학

---

# (AI Math 7강) 통계학 맛보기

---

- 통계적 모델링은 적절한 가정 위에서 확률분포를 추정하는 것이 목표!
- 유한한 개수로만 관찰 할 수 있으니 모집단 분포 정확하게 알 수 없고, 근사적으로 확률분포 추정
- 모수 vs 비모수
    - 모수적 방법: 특정한 확률분포를 따른다고 가정
    - 비모수적 방법: 특정한 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수 개수가 유연하게 바뀌는 경우
        - 비모수적 방법이라고 모수가 없는건 절대 아님!
- 불편 추정량 구하기 위해서 표본분산 구할 땐 n-1로 나눔!
- sampling distribution(표집분포) vs sample distribution(표본분포)
    - sampling distribution(표집분포) : 표본평균과 표본분산의 확률분포
        - 표본평균의 표집분포는 N이 커질수록 중심극한 정리에 의해 정규분포를 따름(모집단의 분포가 정규분포를 따르지 않아도 됨)
    - sample distribution(표본분포) : 확률표본(random sample)의 확률 변수 (random variable)의 분포(distribution)
- 최대가능도(MLE) : 이론적으로 가장 가능성이 높은 모수를 추정하는 방법
    - MLE의 경우 평균은 불편추정량을 보장하지만, 분산의 경우 불편추정량을 보장하지 않음!
    - 가능도함수(Likelihood) : 데이터가 주어져 있는 상황에서 $\theta$가 변하는 함수로 이해(관점의 차이)
        - 가능도함수는 모수  
        $\theta$를 따르는 분포를 관찰할 가능성을 뜻하지만, 확률로 해석하면 안됨!
    - 로그가능도 (Log Likelihood) : 가능도함수에 log!
        - 가능도함수와 로그가능도 함수 모두 적용이 가능, 하지만 가능도함수는 데이터가 커질 경우 계산 불가능
        - 연산 효율성 확보 : $O(n^2)    ->    O(n)$
        - 목적함수를 최소화 하는 것이 목표이므로 음의 로그가능도를 최적화 하여 구함!
        - 미분값이 각 모수에 대해 모두0이 되는 모수를 찾으면 가능도를 최대화 하게 됨!
- 카테고리분포
    - 베르누이 → 이항분포 vs 카테고리 → 다항분포 의 관계!
    - $p_1+ ....... + p_d =1$ 을 만족해야 하므로, 라그랑주 승수법을 통해 최적화 문제를 풀 수 있음
    - 카테고리분포의 MLE는 경우의 수를 세어서 비율을 구하는 것(softmax와 비슷함)
    - softmax vector 는 카테고리 분포의 모수를 모델링
    
    ![https://user-images.githubusercontent.com/46811558/150468860-9f58e1da-1a5b-4124-ab62-2c058f24200b.JPG](https://user-images.githubusercontent.com/46811558/150468860-9f58e1da-1a5b-4124-ab62-2c058f24200b.JPG)
    
                                                              
    
                                                            **위의 이미지 기억하자!**
    
- 확률 분포 사이의 거리
    - 총변동 거리
    - 바슈타인 거리
    - 쿨백-라이블러 발산
        - KL = entropy - cross_entropy
        - 분류문제에서 정답 레이블을 P, 예측 레이블을 Q라 두면 → MLE는 쿨백-라이블러 발산을 최소화 하는 것과 같음!

---

# (AI Math 8강) 베이즈 통계학 맛보기

---

- 조건부확률
    - P(A|B) : 사건 B가 일어난 상황에서 사건 A가 일어날 확률
- 베이즈정리
    
    ![https://user-images.githubusercontent.com/46811558/150469235-a1bdb54c-3e0b-4ccc-9ffd-800a55e212ef.JPG](https://user-images.githubusercontent.com/46811558/150469235-a1bdb54c-3e0b-4ccc-9ffd-800a55e212ef.JPG)
    
    - 베이즈 정리를 통해 새로운 데이터가 들어왔을 때, 앞서 계산한 사후확률을 사전확률로 사용하여 **갱신된 사후 확률을 계산** 할 수 있다!
    - 베이즈의 장점은 데이터가 들어올 때 업데이트하면서 모델링 가능!
- 혼동행렬(Confusion Matrix)
    
    ![https://user-images.githubusercontent.com/46811558/150469337-a0c66282-0b7e-4d23-99de-9e324972d50b.JPG](https://user-images.githubusercontent.com/46811558/150469337-a0c66282-0b7e-4d23-99de-9e324972d50b.JPG)
    
    - 데이터 분석에 따라 제 1종오류에 중점을 둘 지, 2종오류에 중점을 둘 지 다름!
    - Ex : 의료데이터(암)라면, 걸렸는데 안걸렸다 하는(2종오류)것이 더 심각, 안 걸렸는데 걸렸다하면 건강 관리 하겠지!
- 인과관계
    - 인과관계를 알아내기 위해서는 중첩요인의 효과를 제거해야 함!
        - Ex: 키와 IQ의 상관성 → 중첩요인 : 나이 / 나이가 많을 수록 키크고 IQ 높아지겠지!
        
                                                     (중첩효과 제거 안하고 데이터분석하면 엄청 관련 있다고 나옴!)
        
    - 심슨의 역설(심프슨의 역설) : 영국의 통계학자 에드워드 심슨이 정리한 역설!
        - 각 부분에 대한 평균이 크다고 해서 전체애 대한 평균까지 크지는 않다는 의미
        - 각각의 변수에 신경 쓰지 않고 전체 통계 결과를 유추하다 일어나는 오류
        - 함부로 조건부확률로 문제를 정의하면 안됨!

---

# (AI Math 9강) CNN 첫걸음

---

- Convolution
    - 커널은 정의역 내에서 움직여도 변하지 않고, 주어진 신호에 국소적으로 적용함
    - 위치에 따라 바뀌지 않음!
    - 역전파를 계산할 때도 컨볼루션 연산이 나오게 됨(역전파 단계에서 다시 커널을 통해 Gradient가 전달 됨)

---

# (AI Math 10강) RNN 첫걸음

---

- Sequence Data(시퀀스 데이터)
    - 소리,문자열,주가 등의 데이터를 시퀀스 데이터로 분류함
    - 시퀀스 데이터는 i.i.d가 아니기에, 순서를 바꾸거나 과거 정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 됨
    - 이전 시퀀스 정보를 갖고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률 이용가능(베이즈법칙 이용)
    - 시퀀스 데이터를 분석할 때 모든 과거정보들이 필요한 것은 아님
- 예측 모형
    - AR(AutoRegressive model : 자기회귀 모형) : 고정된 길이 $\tau$만큼의 시퀀스만 이용하는 경우
    - 잠재자기회귀모델(Latent AutoRegressive model)
        - AR모형에서 $\tau$가 바뀔 수 있고, 정하는 것 조차 사전지식 필요하기에 제안됨
        - RNN모델의 기본모형
        - 바로 이전 정보를 제외한 나머지 정보들을 $H_t$라는 잠재변수로 인코딩해서 활용하는 모델
    - RNN(Recurrent Neural Network)
        - 잠재자기회귀모델에서, 과거의 정보들의 잠재변수를 인코딩하는 문제가 발생하기에 제안됨
        - 잠재변수 t를 신경망을 통해 반복해서 사용하며 시퀀스데이터의 패턴을 학습하는 모델
        - RNN에서 Weight Matrix는 t에따라 변하지 않음, $H_t$가 변하지!
        - BPTT(BackPropagation Through Time)
            - RNN의 역전파 방법
            - 잠재변수의 연결 그래프에 따라 순차적으로 계산(기울기 소실의 해결책)
            - TRUNCATED BPTT : 시퀀스 길이가 길어지는 경우 BPTT를 통한 역전파 알고리즘의 계산이 불안정해지므로, 길이를 끊는 것이 필요함
        - Vanilla RNN : 길이가 긴 시퀀스 처리하는 데 문제가 있음 → LSTM과 GRU 등장!