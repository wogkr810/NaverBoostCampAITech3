# 이재학

# ****(04강) Convolution은 무엇인가?****

# ****(04강 - 실습) Convolutional Neural Network****

---

- [**알렉스넷 참조 링크**](https://seongkyun.github.io/study/2019/01/25/num_of_parameters/)
    - 풀링,스트라이드 ,패딩은 하이퍼파라미터 (계산 x!)
- Fully-Connected 경우 파라미터수가 큼 → 점점 깊이는 깊어지지만 파라미터 수가 줄어드는 모델 사용(FC Layer 지우고 CNN성능 향상)
- 커널의 경우 꼭 정사각형 아니어도 됨! 직사각형,육각형,별 다 되는 듯!
    - [**HexaConv**](https://openreview.net/forum?id=r1vuQG-CW&fbclid=IwAR2PsxrU8ixqslY3N8JmGuuGmlj1R15y3EGO775ZGAU8LftnoUnGF8pXfxU)
- CNN은 MLP와 다르게 General Gap(Performance : Test Error-Train error)이 없음 , 왜냐면, CNN이 이미지에 갖고 있는 특성때문에
- train 잘해야 함. 왜냐면 추론 때는 BN, DropOut 사용 안하니까

---

# ****(05강) Modern CNN - 1x1 convolution의 중요성****

---

- 알렉스넷(AlexNet)
    - 렐루 사용
    - GPU 2개
    - LRN, Overlapping Pooling
    - Data Augmentation
    - Dropout
- VGG → 3x3 반복!
    - 3x3 filter로 모델의 깊이 쌓음
        - why 3x3 ? : receptive field는 한개의 5x5 = 2개의 3x3 이지만 파라미터수가 훨씬 적음!
    - dropout
    - VGG16 ,VGG19
- GoogLeNet → 1x1 convolution
    - NIN(Network-in-Network) 구조
    - Inception Block → 파라미터 수 줄임 (1x1 filter로 파라미터 수 줄임)
        - 1x1 filter는 채널차원 축소! 채널축소하는데 파라미터 가 별로 없고, 다음꺼 통과할때 는 1x1보다 더 큰 3x3 같은건데, 곱해지는게 작으니까 파라미터 수 확 줄음
- ResNet → skip-connection
    - 모델의 깊이가 깊을수록 학습이 잘 안되는 문제 있음(오버피팅과는 다름)
        - 오버피팅은 파라미터 수가 많을 때 발생
    - identity map(skip connection) 사용
    - bottleneck 구조 사용하여(1x1 이용) 파라미터 수 줄이기도 함
    - 깊이가 깊어지는데, 성능이 좋아지고, 파라미터 수는 줄어들음
- DenseNet → concatenation
    - Resnet은 f(x)+x 느낌이었는데, DenseNet은 concatenation 사용함
        - 전의 layer들을 모두 쌓을수록 무한히 늘어나니까 , 늘어날때마다 중간중간에 Transition Block(BN, 1x1 Conv , 2x2 AvgPooling) 사용하여 차원 줄임
    - [**덴스넷 성능**](https://gaussian37.github.io/dl-concept-densenet/)
- 참고
    - 풀링은 파라미터수 아님(하이퍼파라미터)
    - 5x5 *1 =3x3 *2 → receptive field 관점에서는 같지만 , 25 vs 18 : 파라미터는 더 줄어듦
    - 1x1 채널방향으로 줄이면 다음에 곱하는 필터 사이즈는 더 클 테니, 파라미터 수 줄어 듦
    - [**Degradation 문제**](https://john-baptist.tistory.com/23) : 네트워크 깊이가 증가할수록, 정확도는 포화되다가 급격하게 정확도 감소 → 오버피팅에 의해 발생되는 것이 아닌, 최적화가 쉽지 않다는 것을 보여줌

---

# ****(06강) Computer Vision Applications****

---

- FCN(Fully Convolution Network) : FC layer 없이 Convolution layer로만 구성
    - convolutionalization → FC 레이어와 파라미터 수가 같지만, 마지막까지 convolution으로 구성하여 spatial data를 보존
        - Transforming fully connected layers into convolution layers enables a classification net to output a heat map.
    - upsampling
        - Deconvolution(conv transpose) : 완전 역연산은 아님 / 2+8=3+7=10 이지만, 10으로 역연산하여 2+8인걸 아는 것은 불가능
        - unpooling : avg,max 등등(인공지능 때 배운 것들)
- Detection
    - R-CNN : 이미지 → 2000개의 region → 사이즈 맞추기 → 2000번 CNN(알렉스넷) → SVM으로 분류 ——> 별로..
    - SPPNet : CNN이미지 전체 돌린거에 2000개의 Region 좌표 만 넣자!
        - 2000번은 좀 심하니까.. 다른 방법찾아보자 → CNN 한번만!
        - SPP : Spatial Pyramid Pooling
    - [**Fast R-CNN :**](https://ganghee-lee.tistory.com/36) ROI찾기 → 전체이미지 CNN → SPPnet과 같이 CNN한번 + 프로젝션 시킨 ROI에 대해 특징 찾기 →분류 + 바운딩박스회귀를 통해 박스 위치 조절
    - Faster R-CNN : RPN(Selective Search도 별로니, region 찾는것도 학습 시켜보자) + Fast-RCNN
        - 9*(4+2) → 9개는 3개의 사이즈 * 비율 , 4개는 바운딩박스 4점 , 2개는 박스 사용할지 안할지
    - YOLO(You Only Look Once) : 동시에 박스를 예측하고, 속할 확률을 계산함
        - 굉장히 빠른 알고리즘
        - S X S grid로 나눠서 함
        - S*S*(B*5+C) 사이즈 → S는 그리드, B*5는 사각형 네좌표 + confidence(object의 존재여부), C: number of classes

---

# ****(07강) Sequential Models - RNN****

# ****(07강 - 실습) LSTM****

---

- Sequential Data는 길이가 얼마인지 알 수 없다는 점이 어려움( 몇개의 데이터가 입력으로 들어올지 모르니까.. 하지만 작동해야지)
- 가장 쉽게 → 과거의 몇개만 보겠다.
- markov 모델은 join distribution 을 쉽게 구할 수 있지만, 현실과 너무 다름
    - Ex: 수능 성적은 전날 공부량에만 depend ? ㄴㄴ
- vanilla RNN = RNN
- tensorflow에서 RNN 어려움 but 파이토치는 매우 쉬움!
    - computational graph가 static
    - 버젼 바뀔 때마다 문법바뀜

---

# ****(08강) Sequential Models - Transformer****

# ****(08강 - 실습) SDPA & MHA****

---

- 입력 시퀀스와 출력 시퀀스가 달라도 됨!
- cornerstone(코너스톤) : 주춧돌
- 트랜스포머 → 플렉서블,많이 표현 가능하지만, 메모리많이 먹고 시간오래걸림(N^2)
- abcd vs bacd : 인코딩되는 값은 달라지지 않으니 순서를 주는 positional encoding 필요
- 트랜스포머는 RNN, CNN ㄴㄴ , 병렬화 이용
- 함수안에 , 넣으면 docstring 볼 수 있음