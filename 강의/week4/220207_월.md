# 이재학

# ****(01강) 딥러닝 기본 용어 설명 - Historical Review****

---

- 딥러닝의 중요한 요소!
    - 데이터
    - 모델
    - Loss
    - 알고리즘
- Loss
    - MSE : 회귀
    - Cross-Entropy : 분류
    - MLE : 확률
- proxy : 근사치

---

# ****(02강) 뉴럴 네트워크 - MLP (Multi-Layer Perceptron)****

# ****(02강 - 실습) MLP 구현****

---

- affine 변환 : 점,직선 ,평면을 보존하는 선형 매핑 방식
    - 점 세 개가 변환되면 중점 같은 상대적 위치 보존
- 비선형성이 중요한 이유: 만약 nonlinear가 없다면, 결국 행렬곱이라 b번의 행렬을 연산을 했을때, 행렬이 나오므로 한번 의 행렬 연산을 하는 것만 못함
- 히든레이어는 선형변환의 compact set K에 대해 존재하지만, 내가 하고 있는게 그런건지는 모른다 ㅋ.ㅋ(존재함만 알 수 있음)
- loss가 0이 되게할 경우, 네제곱,제곱,절댓값 모두 0으로 만드는 경우가 같지만, 데이터에 다라 다름. 제곱이라면 오차항에 크게 반응할것임(이상치) , 절댓값은 robust할거임
- 분류에서 다른 레이블보다 값이 크기만하면, 원하는 것으로 할당됨. 따라서 100이든 10000이든 상관없으니 수학적으로 표현이 어렵기에 Cross-Entropy 사용
- super class로 nn.module 받는건 forward 사용위함
- 파이토치는 텐서플로우와 다르게 session이 없기에 편리함
- 파이토치는 computational graph가 dynamic

---

# ****(03강) Optimization****

---

- Generalization Gap : Test error - Train error
- mixup 이상해보이지만 생각보다  잘 올라감!
    - mixup → 이미지를 겹치는 느낌 , 원래 cat :1.0 , dog : 0.0 이었다면, dog : 0.5 , cat : 0.5 같은느낌
    - cutmix → 네이버클로바 에서 함 , mixup처럼 이미지겹치는게아니라, 반반치킨 같은 느낌 , cat : 0.6 , dog : 0.4 같이
- BN은 논란이 있긴한데, 성능이 올라가는건 확실
- cost 최소화 할 때, bias^2+ variance+ noise로 나뉨 → 하나 줄이면 트레이드오프 발생
- boosting vs bagging
    - boosting : 틀린 케이스에 집중(오답노트) → 정확성을 향상
    - bagging : 복원추출하여 여러개의 평균을 냄 → 알고리즘의 안정성
- **[옵티마이저 참고링크](https://yngie-c.github.io/deep%20learning/2020/03/19/training_techs/#fn:1)**
    - GD
    - SGD
    - Momentum
    - NAG
    - Adagrad
    - Adadelta
    - RMSprop
    - Adam
- 트레이닝 시에 랜덤 노이즈 → 예방접종 같은느낌. 일부러 트레이닝할 때 바이러스를 노출 시켜버리기