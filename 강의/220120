- (AI Math 1강) 벡터가 뭐예요? 
  - 벡터(Vector) : 숫자를 원소로 갖는 list 또는 배열
  - 성분곱(Hadamard product : 아다마르 곱) : 성분끼리 곱함
  - Norm : 원점에서부터의 거리
    - L1 : 절댓값 합
    - L2 : sqrt(제곱의 합) -> 삼각부등식 가능 -> 각도계산 가능(수치해석 때 배웠던 중요한 이유) 
    - 왜 다른 norm ? : norm의 종류에 따라 기하학적 성질이 달라짐 / 머신러닝에선 각 성질들에 대해 필요 할 때가 있으므로 둘다 사용함(기하학적 성질에 따라 결정 됨)
 
- (AI Math 2강) 행렬이 뭐예요? 
  - 행렬(Matrix) : 벡터를 워소로 갖는 2차원 배열
  - 연산
    - 곱셈 : i번째 행 벡터 _내적_ j번째 열벡터 <<-- 머리속으로 그리지 말고 이거 생각!
    - np.inner : i번째 행벡터와 j번째 **행**벡터 사이의 내적 (XY^T) <- Transpose 항상 조심
  - Linear Transform의 관점 : 행렬은 벡터공간에서 사용되는 연산자로 이해 가능 -> 행렬곱을 통해 벡터를 다른 차원의 공간으로 보낼 수 있음
  - 역행렬 : 정방행렬이어야 역행렬 가능
    - 수도인버스(Pseudo Inverse, 무어펜로즈 역행렬) : SVD, 회귀분석에서 배웠던 M x N 일 때 역행렬 
   
   
- (AI Math 3강) 경사하강법 - 순한맛 
  - 경사 상승법 : 미분값을 더함 , 함수의 극대값의 위치를 구할 때 사용
  - 경사 하강법 : 미분값을 뺌 , 함수의 극소값의 위치를 구할 때 사용
  - Gradient Vector 에 **-** 붙이면 가장 빨리 감소하게 되는 방향 구할 수 있음
  - 1차원에서는 절댓값으로 기울기를 엡실론과 비교했지만 , Gradient Vector는 Vector 이기에 절댓값 안 됨 -> norm 이용
  
  
- (AI Math 4강) 경사하강법 - 매운맛 
  - 이론적으로 경사하강법은 미분이 가능하고 볼록(convex)한 함수에 대해선 적절한 학습률과 학습횟수를 했을 때 술며이 보장되어 있음
  - 선형회귀 식은 애초에 볼록함수로 정의 -> 수렴!
  - 비선형: 볼록이 보장되지 않기에 문제 발생 -> 경사하강법이 만능이 아니다! -> SGD 제안
  - SGD(Stocastic Gradient Descending : 확률적 경사 하강법)
    - 볼록하지 않은 함수 일때도 가능
    - 모든 데이터를 활용하지 않고 데이터 한개 또는 일부를 활용하여 업데이트 (Minibatch)
    - 원래의 목적함수와 다르게 미니배치에서 목적함수가 바뀔 수 있으므로, 볼록이 아닌 목적함수에서 사용이 가능한 것!
    - 따라서 비선형성이 중요하고 널리 쓰이는 머신러닝 에 효율적이다!
    - 하드웨어적인 관점에서도, imagenet만 봐도 백만장만 주어져도 하드웨어가 감당 못 함 -> 일반적으로 하면 메모리 나가버림 -> 미니배치로 SGD!
    
- (AI Math 5강) 딥러닝 학습방법 이해하기
  - Softmax
    - 소프트맥스를 활용하게 분류문제에 사용
    - 소프트맥스 함수는 모델의 출력을 확률로 해석할 수 있게 변환해줌
    - 학습할 때는 소프트맥스 쓰지만 추론 할 때는 one-hot 사용!
  - 활성화 함수
    - 비선형 함수로서 매우 중요함!
    - 사용하지 않을경우 딥러닝은 선형모형과 차이가 없게 됨.
    - 요즘은 Relue 많이 사용(Relu의 모양을 보고 오해할 수 있지만, 대표적인 비선형 함수)
  - MLP(Multi Layer Perceptron) : 신경망이 여러 층 합성된 함수
    - 층을 왜 여러 개? : 층이 깊을 수록 목적함수를 근사하는 데 필요한 뉴런의 숫자가 훨씬 빨리 줄어들어 좀 더 효율적으로 학습이 가능함
  - 역전파(Backpropagation) : 역전파 알고리즘은 합성함수의 미분법인 연쇄법칙 기반의 자동미분 사용
    - 각 노드의 값을 컴퓨터가 기억해야 하므로 forward보다 메모리 많이 사용
  
- (AI Math 6강) 확률론 맛보기
  - 확률변수 : **확률분포**에 따라 이산형 or 연속형
    - 이산형 확률 변수 : 확률변수가 가질 수 있는 모든 경우의 수를 모두 고려하여 확률을 더해서 모델링
    - 연속형 확률 변수 : 데이터 공간에 정의된 확률변수의 밀도 위에서의 적분을 통해 모델링
      - 밀도는 누적확률분포의 변화율을 모델링하며, 확률로 해석하면 안 됨!
  - 결합분포를 적분 또는 다 더해서 marginal(주변확률분포) 구할 수 있음
  - 조건부 확률(P(Y|X)) : 입력변수 X에 대해 정답이 Y일 확률
    - 연속형 확률 변수에서는 확률이 아닌 밀도
  - **몬테카를로 샘플링** 
    - 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 대부분 -> 데이터를 이용하여 기대값을 계산하려면 몬테카를로 샘플링 방법을 이용해야 함
    - 이산형,연속형 상관없이 성립
    - 독립추출만 보장된다면 , 대수의 법칙에 의해 수렴성을 보장함 
