# 이재학

---

# **(04강) AutoGrad & Optimizer**

---

- Layer, 함수 같은 것들 **Block** 처럼 쌓자 → 블록 반복의 연속
- Torch.nn.Module
    - 딥러닝을 구성하는 Layer의 base class
    - Input, Output, Forward, Backward 정의
    - 학습의 대상이 되는 parameter(tensor) 정의
- nn.Parameter
    - Tensor 객체의 상속 객체
    - **nn.Module 내에 attribute가 될 때는 required_grad=True로 지정되어 학습 대상이 되는 Tensor**
    - 대부분의 layer에는 weights 값들이 지정되어 있기에, 직접 지정할 일은 잘 없음
- Backward
    - Layer에 있는 Parameter들의 미분을 수행
    - Forward의 결과값과 실제값간의 차이(loss)에 대해 미분 수행
    - 해당 값으로 Parameter 업데이트
- **epoch 마다 optimizer.zero_grad()로 초기화 해야 다음 학습에 영향 안 끼침!**

---

# **(05강) Dataset & Dataloader**

---

- Dataset 클래스
    - 데이터 입력 형태를 정의하는 클래스
    - 데이터를 입력하는 방식의 표준화
    - 데이터 형태(Image,Text,Audio) 등에 따른 다른 입력정의
- DataLoader 클래스
    - Data의 Batch를 생성해주는 클래스
    - 학습직전(GPU feed전) 데이터의 변환을 책임
    - Tensor로 변환 + Batch 처리가 메인 업무
    - 병렬적인 데이터 전처리 코드의 고민 필요